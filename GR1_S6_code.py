# -*- coding: utf-8 -*-
"""GR1_code _colab

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RGZwtJSalNvclaJ-M1DIxqjsScr9ErYy
"""

import numpy as np
import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

import time

columns_name=['user_id','item_id','rating','timestamp']
df=pd.read_csv("/content/drive/MyDrive/Data/u.data",sep="\t",names=columns_name)

print(df.head(6))
df.shape

"""*People are given user ids and movies are given item ids. Every person has rated one or more than one movie*
*mais ctte data frame, does not tell which item id corresponds to which movie name.*

*User Ids are assigned to people, while item Ids are assigned to movies. Each person has rated at least one movie.The provided array indicates that the u.data contains the complete database, consisting of 100,000 user ratings (used.id) for movies (item.id). The format of the timestamps in the dataset is also specified.*
"""

movies=pd.read_csv('/content/drive/MyDrive/Data/u.item', sep='|', encoding='latin-1')
print(movies.shape)
movies.head()

"""*The 'u.items' section contains crucial information, including the movie's Id, title, and genres. The last 19 fields indicate whether the film belongs to a particular genre, with '1' indicating that it does and '0' indicating that it does not.*

"""

movies.columns
movies = movies.drop(['01-Jan-1995', 'Unnamed: 3',
       'http://us.imdb.com/M/title-exact?Toy%20Story%20(1995)', '0', '0.1',
       '0.2', '1.1', '1.2', '1.3', '0.3', '0.4', '0.5', '0.6', '0.7', '0.8',
       '0.9', '0.10', '0.11', '0.12', '0.13', '0.14', '0.15'], axis=1)

movies.columns=['item_id','title']
movies.head()

"""*As already mentioned, one user has rated one or more than one movie. This means that one movie has been rated by more than one user.*
*Now, we need the average rating of each movie to get some meaningful data*
"""

df1=pd.merge(df,movies,on="item_id")

df1.groupby("title").mean()['rating'].sort_values(ascending=False)

"""*movies with a rating of exactly 5 or 1 are likely to have been reviewed by only 1 or 2 people because when more number of people rate a movie, the average cannot be a perfect 5 or 1.*"""

df1.groupby("title").count()["rating"].sort_values(ascending=False)



ratings=pd.DataFrame(df1.groupby("title").mean()['rating'])
ratings['number of ratings']=pd.DataFrame(df1.groupby("title").count()["rating"])
print(ratings.head(20))

ratings.sort_values(by='rating', ascending=False)

"""# EDA"""

import matplotlib.pyplot as plt
import seaborn as sns
sns.set_style("dark")

sns.jointplot(x='rating',y='number of ratings',data=ratings,alpha=0.5, color="#4CB391")

from datetime import datetime
dateparse = lambda x: datetime.utcfromtimestamp(int(x)).strftime('%Y-%m-%d %H:%M:%S')

rating_df = pd.read_csv('/content/drive/MyDrive/Data/u.data', sep='\t',
                        names=['user_id', 'movie_id', 'rating', 'timestamp'],
                        parse_dates=['timestamp'],
                        date_parser=dateparse)

movie_df = pd.read_csv('/content/drive/MyDrive/Data/u.item', sep='|', encoding='latin-1',
                    names=['movie_id', 'movie_title', 'release_date', 'video_release_date', 'imdb_url', 'unknown', 'action',
                           'adventure', 'animation', 'childrens', 'comedy', 'crime', 'documentary', 'drama', 'fantasy',
                           'film_noir', 'horror', 'musical', 'mystery', 'romance', 'sci_fi', 'thriller', 'war', 'western'])

user_df = pd.read_csv('/content/drive/MyDrive/Data/u.user', sep='|', encoding='latin-1',
                     names=['user_id', 'age', 'gender', 'occupation', 'zip_code'])

movie_df.sample(10)

movie_df.info()

movie_df.sample(6)

movie_df.describe()

"""**INTERPRETATION**


1. video_release_date seems containing lots of NaN values.
2. release_date needs to be parsed to datetime.
3. imdb_url contains external links which is not usefull here anyways.



"""

movie_df.release_date = pd.to_datetime(movie_df.release_date)

rating_df.info()

rating_df.sample(6)

rating_df.describe()

user_df.info()

user_df.sample(6)

user_df.describe()

user_df['age_group'] = user_df.age.apply(lambda age: 'Gradeschooler' if 5<=age<=12 else ('Teenager' if 13<=age<=19 else ('Young' if 20<=age<=35 else ('Midlife' if 35<=age<=55 else 'Old'))))
user_df.sample(5)

rating_user_df = rating_df.join(other=user_df, how='inner', on='user_id', lsuffix='_R')
rating_user_movie_df = rating_user_df.join(other=movie_df, how='inner', on='movie_id', rsuffix='_M')
rating_movie_df = rating_df.join(other=movie_df, how='inner', on='movie_id', rsuffix='_M')

generes = ['unknown', 'action',
       'adventure', 'animation', 'childrens', 'comedy', 'crime', 'documentary',
       'drama', 'fantasy', 'film_noir', 'horror', 'musical', 'mystery',
       'romance', 'sci_fi', 'thriller', 'war', 'western']

plt.figure(figsize=(12,7))
genere_counts = movie_df.loc[:,generes].sum().sort_values(ascending=False)
sns.barplot(x=genere_counts.index, y=genere_counts.values)
plt.xticks(rotation=60);
print(len(generes))

"""**drama and comedy are the most common genere type. We must also note that it can not be a clear indication of people's preference, because One movie can have more than one genere and drama is the most commor genere type.**"""

plt.figure(figsize=(9,6))
sns.barplot(x=user_df.groupby('gender').size().index, y=user_df.groupby('gender').size().values)
plt.title('Male/Female movie rating ratio');

plt.figure(figsize=(9,6))
sns.barplot(x=user_df.groupby('age_group').size().index,y= user_df.groupby('age_group').size().values)
plt.title('movie watchers age_group wise')
#sns.barplot(x=df.values, y=df.index, alpha=0.8)

plt.figure(figsize=(12,7))
movie_watcher_occupants = user_df.groupby('occupation').size().sort_values(ascending=False)
sns.barplot(x=movie_watcher_occupants.index, y =movie_watcher_occupants.values)
plt.title('movie watchers age_group wise')
plt.xticks(rotation=50)

"""**It appears that Students watches more movies, may be the ones who are above 20 means Young students.**"""

temp_df = rating_user_movie_df.groupby('gender').sum().loc[:,generes]
temp_df = temp_df.transpose()
temp_df

plt.figure(figsize=(12, 6))

temp_df.M.sort_values(ascending=False).plot(kind='bar', color='teal', label="Male")
temp_df.F.sort_values(ascending=False).plot(kind='bar', color='black', label="Fe-Male")
plt.legend()
plt.xticks(rotation=60)
plt.show()

rating_user_df.groupby([rating_user_df.timestamp.dt.year, 'age_group']).size()

temp_df = rating_user_df.groupby(['gender', 'rating']).size()
plt.figure(figsize=(10, 5))
m_temp_df = temp_df.M.sort_values(ascending=False)
f_temp_df = temp_df.F.sort_values(ascending=False)

plt.bar(x=m_temp_df.index, height=m_temp_df.values, label="Male", align="edge", width=0.3, color='teal')
plt.bar(x=f_temp_df.index, height=f_temp_df.values, label="Female", width=0.3, color='black')
plt.title('Ratings given by Male/Female Viewers')
plt.legend()
plt.xlabel('Ratings')
plt.ylabel('Count')
plt.show()

def draw_horizontal_movie_bar(movie_titles, ratings_count, title=''):
    plt.figure(figsize=(12, 7))
    sns.barplot(y=movie_titles, x=ratings_count, orient='h')
    plt.title(title)
    plt.ylabel('Movies')
    plt.xlabel('Count')
    plt.show()

top_ten_rated_movies = rating_movie_df.groupby('movie_id').size().sort_values(ascending=False)[:10]
top_ten_movie_titles = movie_df.iloc[top_ten_rated_movies.index].movie_title

draw_horizontal_movie_bar(top_ten_movie_titles.values, top_ten_rated_movies.values, 'Top 10 watched movies')

user_ratings =df[['user_id','rating']].groupby('user_id').mean()

fig = plt.figure(figsize = (8,8))
user_ratings.plot(kind = 'hist', bins = 100, figsize = (8,8), color = 'blue')
plt.plot()
plt.xlabel('User Ratings')
plt.title('Average User Ratings')
plt.ylabel('Frequency')

"""**From above plot, we can see that on average, user are rating movies 3-4 more frequently than any other rating. This makes sense since people are less inclined to rate movies lower than a 3 if they didn't enjoy the movie. Additionally, I can see the cause of the right-skewed distribution for the average movie ratings per genre.**

**along x-axis, we have the average movie ratings and along the y-axis, we have the number of movies. This is kind of a normal distribution.(:**
"""

ratings = pd.DataFrame(df1.groupby('title')['rating'].mean())
ratings['num of ratings'] = pd.DataFrame(df1.groupby('title')['rating'].count())
sns.jointplot(x='rating',y='num of ratings',data=ratings,alpha=1.0,height = 8)

"""**From the above plot, we see the users are rating mostly 3's and 4's for the movies.**"""

plt.figure(figsize=(12,7))
hhhh = movie_df[movie_df.release_date.dt.year > 1990].groupby(movie_df.release_date.dt.month).size()
sns.barplot(x=['jan', 'feb', 'mar', 'apr', 'may', 'june', 'july', 'aug', 'sept', 'oct', 'nov', 'dec'],y= hhhh.values);
plt.xlabel('Release Month');

plt.figure(figsize=(12,7))
weekday_release_counts = movie_df[movie_df.release_date.dt.year > 1990].groupby(movie_df.release_date.dt.dayofweek).size()
sns.barplot(x=['mon', 'tue', 'wed', 'thus', 'fri', 'sat', 'sun'], y=weekday_release_counts.values);
plt.xlabel('Release Day of Week');

corr = df.corr()
sns.heatmap(corr)

dfF = user_df.drop([ 'age', 'gender', 'zip_code', 'age_group'], axis =1)

dfF.head()

columns_name=['user_id','movie_id','rating','timestamp']
df2=pd.read_csv("/content/drive/MyDrive/Data/u.data",sep="\t",names=columns_name)

df3= movie_df.join(other= dfF, how='inner', on='movie_id', lsuffix='_R')

df3.head()

DF = df3.join(other= df2, how='inner', on='user_id', lsuffix='_R')
DF.head()

DF.columns

DF1= DF.drop([ 'movie_title', 'release_date', 'video_release_date',
       'imdb_url', 'rating', 'timestamp'], axis = 1)

DF1.head()

f, ax = plt.subplots(1,1, figsize=(12, 8))
plt.title("Users age distribution distribution, grouped by occupation (943 users, 21 occupations)")
sns.kdeplot(data=user_df, x="age", hue="occupation", levels=20)
plt.show()

f, ax = plt.subplots(1,1, figsize=(6, 6))
plt.title("Users age distribution, grouped by gender (943 users)")
sns.kdeplot(data=user_df, x="age", hue="gender", levels=20)
plt.show()

f, ax = plt.subplots(1,1, figsize=(8, 8))
plt.title("Ratings number and ratings mean distribution (100K ratings)")
sns.kdeplot(data=ratings, x="rating", y="num of ratings", levels=20)
plt.show()

dp = pd.DataFrame(DF1)


liste_de_listes = dp.values.tolist()
print(len(liste_de_listes))

print(liste_de_listes)

def liste_de_film(x , y):
  for i in range(len(liste_de_listes)):
    if liste_de_listes[i][0]==x:
      if liste_de_listes[i][-3]== y :return liste_de_listes[i][1:20]
      else:return [0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]

def somme(y):
  A=[liste_de_film(i,y) for i in range(len(liste_de_listes))]
  colonnes_groupées = zip(*A[1:])
  somme_colonnes = [sum(colonne) for colonne in colonnes_groupées ]
  return somme_colonnes

somme('educator')
len(somme('educator'))

generes = [ 'unknown','action',
       'adventure', 'animation', 'childrens', 'comedy', 'crime', 'documentary',
       'drama', 'fantasy', 'film_noir', 'horror', 'musical', 'mystery',
       'romance', 'sci_fi', 'thriller', 'war', 'western']
occupation = ['technician', 'other', 'writer', 'executive', 'administrator', 'student', 'lawyer', 'educator', 'scientist', 'entertainment', 'programmer', 'librarian', 'homemaker', 'artist', 'engineer', 'marketing', 'none', 'healthcare', 'retired', 'salesman', 'doctor']
print(len(generes))

def occupation_most_watched_movie(a):
  plt.figure(figsize=(5,4))
  sns.barplot(x=generes, y=somme(a))
  plt.title(a)
  plt.xticks(rotation=60);

for i in range(len(occupation)):
 occupation_most_watched_movie(occupation[i])

"""# K-means"""

userr = pd.read_csv('/content/drive/MyDrive/Data/u.user', delimiter='|', header=None,
                      names=['user_id',"age","gender","occupation","zip_code"])
userr.head()

userrr=userr.drop(["zip_code"],axis=1)
userrr.head()

usere=userrr.replace(to_replace=["M", "F"],
           value=[1,0])
usere.head()

userf=usere.replace(to_replace=occupation,
           value=[i for i in range(21)])
userf.head()

"""**One hot encoding**"""

user_num = pd.get_dummies(usere, columns=['occupation'])
user_num.head()

user_df = user_num.drop("user_id", axis = 1)
user_df

item = pd.read_csv("/content/drive/MyDrive/Data/u.item", sep = "|", names = ["movie_id", "movie title", "release date", "video release date",
              "IMDb URL", "unknown", "Action", "Adventure", "Animation",
              "Children's", "Comedy", "Crime", "Documentary", "Drama", "Fantasy",
              "Film-Noir", "Horror", "Musical", "Mystery", "Romance", "Sci-Fi",
              "Thriller", "Wa", "Western"], encoding="latin-1")
item.head()

del item["movie title"]
del item["release date"], item["video release date"], item["IMDb URL"], item["unknown"],

item_df = item.drop("movie_id", axis = 1)

"""**1.Silhouette user**"""

from sklearn.preprocessing import MinMaxScaler

ms = MinMaxScaler()

user_scaled = ms.fit_transform(user_df)

from sklearn.metrics import silhouette_score
def silhouette(user_scaled, dist):
    silhouette_scores = []
    ks = range(2, 30)  # Le score de silhouette n'est pas défini pour k=1
    for k in ks:
        kmeans = KMeans(n_clusters=k, random_state=0, init='k-means++', max_iter= 300,  )
        cluster_labels = kmeans.fit_predict(user_scaled)
        silhouette_avg = silhouette_score(user_scaled, cluster_labels)
        silhouette_scores.append(silhouette_avg)

    # Tracer le score de silhouette pour différents k
    plt.figure(figsize=(10, 6))
    plt.plot(ks, silhouette_scores, '-o')
    plt.xlabel('Nombre de clusters, k')
    plt.ylabel('Score de Silhouette')
    plt.xticks(ks)
    plt.title('Score de Silhouette pour différents nombres de clusters')
    plt.show()

k = 20
# Application de K-means avec MiniBatchKMeans pour les grands jeux de données

kmeans = KMeans(n_clusters = k, random_state=0, init='k-means++', max_iter = 300)
user_clusters = kmeans.fit_predict(user_scaled)

# Calcul du Silhouette Score
score = silhouette_score(user_scaled, user_clusters)
print(f'Silhouette Score: {score}')

# Réduction de dimensionnalité à 2D pour la visualisation
from sklearn.decomposition import PCA


pca = PCA(n_components=2)
user_df_reduced = pca.fit_transform(user_scaled)

# Visualisation
plt.figure(figsize=(10, 7))
plt.scatter(user_df_reduced[:, 0], user_df_reduced[:, 1], c=user_clusters, cmap='viridis', marker='o', edgecolor='k', s=50, alpha=0.7)
plt.title('Visualisation des Clusters d\'Utilisateurs')
plt.xlabel('Composante principale 1')
plt.ylabel('Composante principale 2')

# Centres de clusters
centers = pca.transform(kmeans.cluster_centers_)
plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.5, marker='X')

plt.show()

"""**2.Silhouette**"""

item_scaled = ms.fit_transform(item_df)

from sklearn.metrics import silhouette_score

silhouette_scores = []
ks = range(2, 51)  # Le score de silhouette n'est pas défini pour k=1
for k in ks:
    kmeans1 = KMeans(n_clusters=k, random_state=0, init='k-means++')
    cluster_labels = kmeans1.fit_predict(item_scaled)
    silhouette_avg = silhouette_score(item_scaled, cluster_labels)
    silhouette_scores.append(silhouette_avg)

# Tracer le score de silhouette pour différents k
plt.figure(figsize=(10, 6))
plt.plot(ks, silhouette_scores, '-o')
plt.xlabel('Nombre de clusters, k')
plt.ylabel('Score de Silhouette')
plt.xticks(ks)
plt.title('Score de Silhoue')

print(np.argmax(silhouette_scores))

k = 48

kmeans2 = KMeans(n_clusters = k, random_state=0, init='k-means++', max_iter= 100)
item_clusters = kmeans2.fit_predict(item_scaled)

# Calcul du Silhouette Score
score = silhouette_score(item_scaled, item_clusters)
print(f'Silhouette Score: {score}')

# Réduction de dimensionnalité à 2D pour la visualisation
from sklearn.decomposition import PCA


pca = PCA(n_components=2)
item_df_reduced = pca.fit_transform(item_scaled)

# Visualisation
plt.figure(figsize = (10, 7))
plt.scatter(item_df_reduced[:, 0], item_df_reduced[:, 1], c=item_clusters, cmap='viridis', marker='o', edgecolor='k', s=50, alpha=0.7)
plt.title('Visualisation des Clusters des items')
plt.xlabel('Composante principale 1')
plt.ylabel('Composante principale 2')

# Centres de clusters
centers = pca.transform(kmeans2.cluster_centers_)
plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.5, marker='X')

plt.show()

"""**Elbow**"""

inertia = []
df = pd.DataFrame(userf)
liste_de_user = df.values.tolist()
nplu=np.array(liste_de_user)
list_num_clusters = list(range(1,20))
for num_clusters in list_num_clusters:
    km = KMeans(n_clusters=num_clusters)
    km.fit(nplu)
    inertia.append(km.inertia_)

plt.plot(list_num_clusters,inertia)
plt.scatter(list_num_clusters,inertia)
plt.xlabel('Number of Clusters')
plt.ylabel('Inertia');

# Second implementation of K-means with k-means++ initialization
start_time_kmeansPP = time.time()
# Application de K-means
k = 10  # Exemple de nombre de clusters
kmeans = KMeans(n_clusters=k, random_state=0, init= "k-means++")
user_clusters = kmeans.fit_predict(user_df)
end_time_kmeansPP = time.time()
execution_time_kmeansPP = end_time_kmeansPP - start_time_kmeansPP

# user_clusters contient les indices de clusters pour chaque utilisateur
print(execution_time_kmeansPP,'s')
user_clusters

# Application de K-means
# First implementation of K-means with default initialization
start_time_default = time.time()
k = 10  # Exemple de nombre de clusters
kmeans = KMeans(n_clusters=k, random_state=0)
user_clusters1 = kmeans.fit_predict(user_df)
end_time_default = time.time()
execution_time_default = end_time_default - start_time_default
# user_clusters contient les indices de clusters pour chaque utilisateur
print(execution_time_default,'s')
user_clusters1

from sklearn.decomposition import PCA


pca = PCA(n_components=2)
user_df_reduced = pca.fit_transform(user_df)

# Visualisation
plt.figure(figsize=(12, 7))
plt.scatter(user_df_reduced[:, 0], user_df_reduced[:, 1], c=user_clusters, cmap='viridis', marker='o', edgecolor='k', s=50, alpha=0.7)
plt.title('Users clusters visualisation ')
plt.xlabel('PRINCIPALE COMPONENT1')
plt.ylabel('PRINCIPALE COMPONENT2')

# Centres de clusters
centers = pca.transform(kmeans.cluster_centers_)
plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.5, marker='X')

plt.show()

from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler



# Application de K-means
k = 15  # Exemple de nombre de clusters
kmeans = KMeans(n_clusters=k, random_state=0, init= "k-means++")
item_clusters = kmeans.fit_predict(item_df)

# user_clusters contient les indices de clusters pour chaque utilisateur

item_clusters

k = 15  # Exemple de nombre de clusters
kmeans = KMeans(n_clusters=k, random_state=0)
item_clusters1 = kmeans.fit_predict(item_df)

# user_clusters contient les indices de clusters pour chaque utilisateur

item_clusters1

from sklearn.decomposition import PCA


pca = PCA(n_components=2)
item_df_reduced = pca.fit_transform(item_df)

# Visualisation
plt.figure(figsize=(10, 7))
plt.scatter(item_df_reduced[:, 0], item_df_reduced[:, 1], c=item_clusters, cmap='viridis', marker='o', edgecolor='k', s=50, alpha=0.7)
plt.title('Movies clusters visualisation')
plt.xlabel('Principale component1')
plt.ylabel('Principale component2')

# Centres de clusters
centers = pca.transform(kmeans.cluster_centers_)
plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, alpha=0.5, marker='X')

plt.show()

rating_df.head()

# Créer une colonne pour les clusters d'utilisateurs et de films dans le DataFrame original
rating_df['user_cluster'] = rating_df['user_id'].apply(lambda x: user_clusters[x-1])
rating_df['movie_cluster'] = rating_df['movie_id'].apply(lambda x: item_clusters[x-1])
rating_df.head()

# Calculer la moyenne de rating pour chaque combinaison de clusters
cluster_means = rating_df.groupby(['user_cluster', 'movie_cluster'])['rating'].mean()
print(cluster_means)

ratings_matrix = rating_df.pivot_table(index='user_id', columns='movie_id', values='rating')
ratings_matrix.head()

# Itérer sur chaque élément de la matrice de ratings
for user_id in range(1, len(user_clusters) + 1):
    for movie_id in range(1, len(item_clusters) + 1):
        # Vérifier si le rating est manquant
        if np.isnan(ratings_matrix.at[user_id, movie_id]):
            user_clust = user_clusters[user_id - 1]
            movie_clust = item_clusters[movie_id - 1]
            # Remplacer par la moyenne du cluster si elle existe
            if (user_clust, movie_clust) in cluster_means.index:
                ratings_matrix.at[user_id, movie_id] = cluster_means[user_clust, movie_clust]
            else:
                # Optionnel : utiliser une valeur par défaut si aucune moyenne de cluster n'est disponible
                ratings_matrix.at[user_id, movie_id] = rating_df['rating'].mean()

ratings_matrix.head()

DFF = pd.DataFrame(ratings_matrix)


liste_de_movief = DFF.values.tolist()
npmf=np.array(liste_de_movief)

print(npmf)
print(npmf.shape)
print(len(npmf))

def topp10(ma_liste):
    valeurs_indices = [(valeur, index) for index, valeur in enumerate(ma_liste)]

    # Trier les valeurs par ordre décroissant, en cas d'égalité, utiliser l'index d'origine
    valeurs_indices_triees = sorted(valeurs_indices, key=lambda x: (-x[0], x[1]))

    # Extraire les 10 premiers indices des valeurs triées
    indices_des_10_premiers = [index for valeur, index in valeurs_indices_triees[:10]]

    return indices_des_10_premiers

def topindic(u,matrice):
     l=list(matrice[u-1])
     l2=topp10(l)
     return [l2[i]+1 for i in range(10)]

topindic(1,npmf)

item1 = pd.read_csv("/content/drive/MyDrive/Data/u.item", sep = "|", names = ["movie_id", "movie title", "release date", "video release date",
              "IMDb URL", "unknown", "Action", "Adventure", "Animation",
              "Children's", "Comedy", "Crime", "Documentary", "Drama", "Fantasy",
              "Film-Noir", "Horror", "Musical", "Mystery", "Romance", "Sci-Fi",
              "Thriller", "Wa", "Western"], encoding="latin-1")
item1.head()

gg = pd.DataFrame(item1)


hh = gg.values.tolist()
npm=np.array(hh)

print(npm)
print(npm.shape)
print(len(npm))

def topmovies(u,matrice):
    l=topindic(u,matrice)
    return [npm[u-1][1]for u in l]

topmovies(1,npmf) # the list of movies recommended to user who has a id=1

"""**The process begins with loading the training data, followed by applying the pivot method to transform the data frame into a rating matrix, where rows represent user_ids and columns represent item_ids. Next, the "matrice_remplie_de_la_data" function is utilized to fill in missing ratings. This can be accomplished either through a mean-based method—by grouping user_clusters with item_clusters and calculating the average rating for each combination (once this grouping, or cluster_means, is achieved, the matrix is traversed, and each missing rating is replaced with its corresponding value found in cluster_means)—or by employing the "most frequent rating" method, following the same steps as the mean method. After filling in the matrix, it's crucial to evaluate the model's performance using metrics such as the F1-score, RMSE, MAE, precision, and recall. Prior to this evaluation, a numpy array is created with two columns: the first contains the actual ratings, and the second contains the predicted ratings.**"""

def matrice_remplie_de_la_data(u,methode):


    # Load rating data
    rating_df = pd.read_csv(f'/content/drive/MyDrive/Data/{u}', sep='\t',
                            names=['user_id', 'movie_id', 'rating', 'timestamp'])

    # Assign clusters to ratings
    rating_df['user_cluster'] = rating_df['user_id'].apply(lambda x: user_clusters[x-1])
    rating_df['movie_cluster'] = rating_df['movie_id'].apply(lambda x: item_clusters[x-1])
    # Rating_matrix using pivot
    ratings_matrix = rating_df.pivot_table(index='user_id', columns='movie_id', values='rating')
    if methode =="mean":
      # Calculate mean ratings for each cluster combination
      cluster_means = rating_df.groupby(['user_cluster', 'movie_cluster'])['rating'].mean()
      # Fill in missing ratings based on cluster averages
      for user_id in ratings_matrix.index:
          for movie_id in ratings_matrix.columns:
              if pd.isnull(ratings_matrix.at[user_id, movie_id]):
                  user_clust = rating_df[rating_df['user_id'] == user_id]['user_cluster'].iloc[0]
                  movie_clust = rating_df[rating_df['movie_id'] == movie_id]['movie_cluster'].iloc[0]
                  if (user_clust, movie_clust) in cluster_means.index:
                      ratings_matrix.at[user_id, movie_id] = cluster_means[user_clust, movie_clust]
                  else:
                      ratings_matrix.at[user_id, movie_id] = rating_df['rating'].mean()
    if methode == "most_freq":
      default_rating1 = rating_df['rating'].mean()
      # Calculate most frequent rating for each cluster combination
      from scipy.stats import mode
      cluster_modes1 = rating_df.groupby(['user_cluster', 'movie_cluster'])['rating'].agg(lambda x: x.mode()[0] if not x.mode().empty else np.nan)
      # Transformer le résultat en DataFrame pour une meilleure lisibilité
      cluster_modes_df1 = cluster_modes1.reset_index()
      cluster_modes_df1.rename(columns={'rating': 'most_frequent_rating'}, inplace=True)
      for user_id in ratings_matrix.index:
          for movie_id in ratings_matrix.columns:
              # Vérifier si le rating est manquant
              if pd.isnull(ratings_matrix.at[user_id, movie_id]):
                  # Trouver les clusters correspondants pour cet utilisateur et ce film
                  user_clust = user_clusters[user_id - 1]  #
                  movie_clust = item_clusters[movie_id - 1]  #

                  # Extraire le rating le plus fréquent pour cette combinaison de clusters
                  mode_rating = cluster_modes_df1[
                      (cluster_modes_df1['user_cluster'] == user_clust) &
                      (cluster_modes_df1['movie_cluster'] == movie_clust)
                  ]['most_frequent_rating'].values

                  # Si un rating le plus fréquent est trouvé, l'utiliser; sinon, utiliser la moyenne globale
                  if mode_rating.size > 0:
                      ratings_matrix.at[user_id, movie_id] = mode_rating[0]
                  else:
                      ratings_matrix.at[user_id, movie_id] = default_rating1

    return ratings_matrix

matrice_u1=matrice_remplie_de_la_data("u1.base","mean")#it's take a lot of time  that's why we expirement just u1.base and u1.test

"""**RMSE, MAE , f1 score and Recall**"""

test_dff = pd.read_csv("/content/drive/MyDrive/Data/u1.test", sep = "\t", names = ["user_id", "movie_id", "rating", "timestamp"])
del test_dff["timestamp"]
R = test_dff.to_numpy()
from sklearn.metrics import f1_score
from sklearn.metrics import mean_squared_error
from math import sqrt


def test_predict(R, M):
    predicted_rating = np.zeros((R.shape[0], 2))
    for i in range(R.shape[0]):
        user_id = R[i, 0]
        movie_id = R[i, 1]
        true_rating = R[i, 2]

        # Vérifier si les indices sont valides, si non, continuer à l'itération suivante
        if user_id not in M.index or movie_id not in M.columns:
            continue

        predicted_rating[i] = [true_rating, M.at[user_id, movie_id]]

    return predicted_rating
N = test_predict(R, matrice_u1)

# RMSE Metric
rmse = sqrt(mean_squared_error(N[:, 0], N[:,1]))


def f1score(test_predict):
# Exemple de seuil
  seuil =3.5

# Transformer les ratings réels
  y_true = (test_predict[:,0] >= seuil).astype(int)

# Transformer les ratings prédits (s'assurer que test_df contient les prédictions)
  y_pred = (test_predict[:,1] >= seuil).astype(int)

# Calculer le F1-score
  f1 = f1_score(y_true, y_pred)

  return f'F1-Score:{f1}'
f1score(N)

from sklearn.metrics import recall_score

# Exemple de seuil pour définir un rating positif
seuil = 3.5

# Transformer les ratings réels en un vecteur binaire (1 pour positif, 0 pour négatif)
y_true = (N[:,0] >= seuil).astype(int)

# Transformer les ratings prédits de la même manière
y_pred = (N[:,1] >= seuil).astype(int)

# Calculer le rappel
recall = recall_score(y_true, y_pred)

print(f'Recall: {recall}')

from sklearn.metrics import precision_score

# Transformer les ratings en labels binaires comme pour le rappel et le F1-score
y_true = (N[:,0] >= seuil).astype(int)
y_pred = (N[:,1] >= seuil).astype(int)

# Calculer la précision
precision = precision_score(y_true, y_pred)
print(f'Precision: {precision}')



"""# Let's start modeling KNN

"""

df.head(2)

movie_df.head(2)

movies.head()

columns_name=['user_id','item_id','rating','timestamp']
dfo=pd.read_csv("/content/drive/MyDrive/Data/u.data",sep="\t",names=columns_name)

d = pd.merge(dfo, movies, on='item_id')
d.head()

d.drop(["timestamp"], axis = 1, inplace = True)

d.head(2)

"""**Let's Set Up Our Recommendation System**


*   Firstly, we set up a structure similar to the pivot table in Excel.
*   According to this structure, each row will be a user (so the index of our data frame will be user_id)
*   The columns will have film names (i.e. the title column),
We create a data frame with rating values in the table!



"""

moviemat = d.pivot_table(index='user_id',columns='title',values='rating').fillna(0)
moviemat.head(7)

# We choose random movie.
iDx = np.random.choice(moviemat.shape[0])
print("Choosen Movie is: ",moviemat.index[iDx])

from scipy.sparse import csr_matrix
from sklearn.neighbors import NearestNeighbors
user_movie_table_matrix = csr_matrix(moviemat.values)
model_knn = NearestNeighbors(metric = 'cosine', algorithm = 'brute')
model_knn.fit(user_movie_table_matrix)

distances, indices = model_knn.kneighbors(moviemat.iloc[iDx,:].values.reshape(1,-1), n_neighbors = 6)
distances

indices

"""

1.   user_movie_table_matrix = csr_matrix(user_movie_table.values): Here, user_movie_table is assumed to be a DataFrame where rows represent users and columns represent movies (or items), and the cells contain some measure of interaction or preference, like ratings. csr_matrix is a function from the SciPy library that converts the DataFrame into a Compressed Sparse Row matrix, which is an efficient way to store large sparse matrices.

2.  model_knn = NearestNeighbors(metric='cosine', algorithm='brute'): This line initializes a KNN model. NearestNeighbors is a class from scikit-learn used for finding nearest neighbors based on a distance metric. In this case, 'cosine' is specified as the metric, indicating that cosine similarity will be used to measure the distance between user vectors. 'brute' specifies the brute-force algorithm for nearest neighbor search, which is straightforward but can be computationally expensive for large datasets.

3.  model_knn.fit(user_movie_table_matrix): This line fits the KNN model to the data. It calculates distances between all pairs of points in the dataset and stores them for future queries. Here, user_movie_table_matrix is passed as the data to be fitted.

4.  distances, indices = model_knn.kneighbors(user_movie_table.iloc[query_index,:].values.reshape(1,-1), n_neighbors=6): This line finds the k-nearest neighbors for a given query point.

5.  user_movie_table.iloc[query_index, :] selects the row in the DataFrame corresponding to the user at query_index. .values.reshape(1,-1) converts the selected row into a 2D NumPy array with a single row and multiple columns, which is the format expected by scikit-learn models. model_knn.kneighbors() is then called to find the nearest neighbors for this query point. n_neighbors=6 specifies that it should find the 6 nearest neighbors.


"""

movie = []
distance = []

for i in range(0, len(distances.flatten())):
    if i != 0:
        movie.append(moviemat.T.index[indices.flatten()[i]])
        distance.append(distances.flatten()[i])

m=pd.Series(movie,name='movie')
d=pd.Series(distance,name='distance')
recommend = pd.concat([m,d], axis=1)
recommend = recommend.sort_values('distance',ascending=False)

print('Recommendations for {0}:\n'.format(moviemat.T.index[iDx]))
for i in range(0,recommend.shape[0]):
    print('{0}: {1}, with distance of {2}'.format(i, recommend["movie"].iloc[i], recommend["distance"].iloc[i]))

"""# Collaborative Filtering Another Models
**Here we evaluate one by one using ui.base et ui.test  for i in range(1,6)**
"""

!pip install surprise

from surprise import Dataset, Reader, NMF, SVDpp, KNNBasic,SVD ,accuracy
from surprise.model_selection import PredefinedKFold
import os

# Chemin de base pour les fichiers, à ajuster selon vos besoins
base_path = os.path.expanduser('/content/drive/MyDrive/Data/')

# Initialisation du lecteur avec le format de ligne et le séparateur appropriés
reader = Reader(line_format='user item rating timestamp', sep='\t')

# Définition des algorithmes à utiliser
algorithms = {
    'NMF': NMF(),
    'SVD': SVD(),
    'SVD++': SVDpp(),
    'KNNBasic': KNNBasic()

}

def print_results(algo_name, results):
    rmse_scores = results['rmse']
    mae_scores = results['mae']
    fit_times = results['fit_time']
    test_times = results['test_time']

    print(f"Evaluating RMSE and MAE of algorithm {algo_name} on 5 split(s).\n")
    print(f"{'Fold':<10} {'RMSE':>10} {'MAE':>10} {'Fit time':>10} {'Test time':>10}")
    for i in range(5):
        print(f"{i+1:<10} {rmse_scores[i]:>10.4f} {mae_scores[i]:>10.4f} {fit_times[i]:>10.4f} {test_times[i]:>10.4f}")
    print(f"{'Mean':<10} {np.mean(rmse_scores):>10.4f} {np.mean(mae_scores):>10.4f} {np.mean(fit_times):>10.4f} {np.mean(test_times):>10.4f}")
    print(f"{'Std':<10} {np.std(rmse_scores):>10.4f} {np.std(mae_scores):>10.4f} {np.std(fit_times):>10.4f} {np.std(test_times):>10.4f}")
    print("\n")

# Boucle sur chaque algorithme
for name, algo in algorithms.items():
    results = {
        'rmse': [],
        'mae': [],
        'fit_time': [],
        'test_time': []
    }

    # Boucle sur chaque ensemble de fichiers .base et .test
    for i in range(1, 6):  # u1 à u5
        # Chemins vers les fichiers pour le pli courant
        train_file = f'{base_path}u{i}.base'
        test_file = f'{base_path}u{i}.test'

        # Chargement des données de train et test
        data = Dataset.load_from_folds([(train_file, test_file)], reader=reader)
        pkf = PredefinedKFold()

        # Évaluation de l'algorithme sur le pli courant
        for trainset, testset in pkf.split(data):
            start_fit = time.time()
            algo.fit(trainset)
            end_fit = time.time()
            predictions = algo.test(testset)
            end_test = time.time()

            # Calculer et ajouter les scores et les temps
            rmse = accuracy.rmse(predictions, verbose=False)
            mae = accuracy.mae(predictions, verbose=False)
            results['rmse'].append(rmse)
            results['mae'].append(mae)
            results['fit_time'].append(end_fit - start_fit)
            results['test_time'].append(end_test - end_fit)

    # Imprimer les résultats pour l'algorithme actuel
    print_results(name, results)

"""**MORE DETAILS ABOUT SVD MODEL**"""

from scipy.sparse.linalg import svds
from scipy.sparse import csc_matrix

# Supposons que 'matrix' est déjà converti en une matrice creuse, par exemple :
# matrix_sparse = csc_matrix(matrix)
def yy(g):
  g=g.fillna(0)
  ggg=g.to_numpy()
  # Appliquer SVD pour k valeurs singulières, où k est plus petit que la dimension de la matrice
  k = min(ggg.shape) - 1  # ou toute autre valeur plus petite appropriée
  U, Sigma, VT = np.linalg.svd(ggg, full_matrices=False)

# Convertir Sigma en une matrice diagonale avec la bonne dimension
  Sigma_mat = np.zeros((U.shape[1], VT.shape[0]))
  np.fill_diagonal(Sigma_mat, Sigma)

  # Calculer le produit pour reconstituer la matrice originale
  reconstructed_matrix = np.dot(U, np.dot(Sigma_mat, VT))
  return  reconstructed_matrix

rating_dfff = pd.read_csv("/content/drive/MyDrive/Data/u1.base", sep='\t',names=['user_id', 'movie_id', 'rating', 'timestamp'])

rating_dfff['user_cluster'] = rating_dfff['user_id'].apply(lambda x: user_clusters[x-1])
rating_dfff['movie_cluster'] = rating_dfff['movie_id'].apply(lambda x: item_clusters[x-1])

    # Calculate mean ratings for each cluster combination
cluster_means = rating_dfff.groupby(['user_cluster', 'movie_cluster'])['rating'].mean()
ratings_matrix = rating_dfff.pivot_table(index='user_id', columns='movie_id', values='rating')
yy(matrice_u1)
MATR=pd.DataFrame(yy(matrice_u1),[i for i in range(943)])
MATR

f1score(test_predict(R, MATR))